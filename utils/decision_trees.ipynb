{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import opendatasets as od\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
    "subdir = 'utils'\n",
    "path = os.path.join(parent_dir, subdir)\n",
    "sys.path.append(path)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:Your Kaggle Key:Downloading titanic.zip to ./titanic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34.1k/34.1k [00:00<00:00, 507kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting archive ./titanic/titanic.zip to ./titanic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the data from here https://www.kaggle.com/competitions/nyc-taxi-trip-duration/data\n",
    "path_to_directory = os.path.abspath('../data/titanic')\n",
    "if not os.path.exists(path_to_directory):\n",
    "    # URL of the dataset\n",
    "    dataset_url = 'https://www.kaggle.com/competitions/titanic/data'\n",
    "    # Download the dataset to the specified location\n",
    "    od.download(dataset_url)\n",
    "    shutil.move(os.path.abspath('titanic'), path_to_directory)\n",
    "\n",
    "train_data = 'train.csv'\n",
    "test_data = 'test.csv'\n",
    "train_df = pd.read_csv(os.path.join(path_to_directory, train_data))\n",
    "test_df = pd.read_csv(os.path.join(path_to_directory, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Treest\n",
    "\n",
    "Recursive partitioning. Regularisation reduce the depth of the tree.\n",
    "\n",
    "Advantages:\n",
    "- white box models\n",
    "\n",
    "Disadvantages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision trees\n",
    "\n",
    "class Decision_tree():\n",
    "    def __init__(self, data, target, max_depth) -> None:\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.seen = set()\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def all_iteration(self):\n",
    "        while self.max_depth >0:\n",
    "            self.best_split_for_single_iter()\n",
    "            print(self.seen)\n",
    "            self.max_depth -= 1 \n",
    "        return self.gini_values\n",
    "        \n",
    "    def best_split_for_single_iter(self):\n",
    "        columns = [col for col in self.data.columns if col not in ['PassengerId', self.target, 'Name', 'Ticket'] ]\n",
    "        self.gini_values = {}\n",
    "        for col  in columns:\n",
    "            for val in self.data[col].unique():\n",
    "                if (col, str(val)) not in self.seen:\n",
    "                    gini_weighted = self.split(col, val)\n",
    "                    self.gini_values[col + '_split_' + str(val)] = gini_weighted\n",
    "                else:\n",
    "                    print('dont calculate gini on these', col, val)\n",
    "                    \n",
    "        # Find the minimum value of the dictionary and update the self.data\n",
    "        min_key = min(self.gini_values, key=self.gini_values.get)\n",
    "        col, val = min_key.split('_split_')\n",
    "        print(col, val)\n",
    "        self.seen.add((col, str(val)))\n",
    "        return (col, val)\n",
    "        \n",
    "            \n",
    "    def split(self, col, val):\n",
    "        \n",
    "        # Calculate probability for each node\n",
    "        cls1 = self.data[self.data[col] == val]\n",
    "        cls2 = self.data[self.data[col] != val]\n",
    "        prob_list, n1, n2 = self.calculate_probability(cls1, cls2)\n",
    "        \n",
    "        # Calculate gini_index for each node\n",
    "        g1, g2 = self.gini_index(prob_list)\n",
    "        \n",
    "        # Return weighted gini-impuritiny index\n",
    "        return self.gini_impurity_for_split(g1, g2, n1, n2)\n",
    "    \n",
    "    def calculate_probability(self, cls1, cls2):\n",
    "        ''''''\n",
    "        prob_cls1 = []\n",
    "        prob_cls2 = []\n",
    "        n1 = len(cls1)\n",
    "        n2 = len(cls2)\n",
    "        for val in cls1[self.target].unique():\n",
    "            temp_data = len(cls1[cls1[self.target] == val].copy(deep=True))\n",
    "            prob_cls1.append(temp_data / n1)\n",
    "\n",
    "        for val in cls2[self.target].unique():\n",
    "            value2 = len(cls2[cls2[self.target] == val].copy(deep=True))\n",
    "            prob_cls2.append(value2 / n2)\n",
    "        return [prob_cls1, prob_cls2], n1, n2\n",
    "        \n",
    "    def gini_index(self, prob_list):\n",
    "        g1 =  1 - np.sum([element **2 for element in prob_list[0]])\n",
    "        g2 =  1 - np.sum([element **2 for element in prob_list[1]])\n",
    "        return g1, g2\n",
    "        \n",
    "    def gini_impurity_for_split(self, g1, g2, n1, n2):\n",
    "        return n1/(n1+n2) * g1 + n2/(n1+n2) * g2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex male\n",
      "{('Sex', 'male')}\n",
      "dont calculate gini on these Sex male\n",
      "Sex female\n",
      "{('Sex', 'male'), ('Sex', 'female')}\n",
      "dont calculate gini on these Sex male\n",
      "dont calculate gini on these Sex female\n",
      "Pclass 3\n",
      "{('Sex', 'male'), ('Sex', 'female'), ('Pclass', '3')}\n",
      "dont calculate gini on these Pclass 3\n",
      "dont calculate gini on these Sex male\n",
      "dont calculate gini on these Sex female\n",
      "Cabin 0\n",
      "{('Sex', 'male'), ('Cabin', '0'), ('Sex', 'female'), ('Pclass', '3')}\n",
      "dont calculate gini on these Pclass 3\n",
      "dont calculate gini on these Sex male\n",
      "dont calculate gini on these Sex female\n",
      "dont calculate gini on these Cabin 0\n",
      "Pclass 1\n",
      "{('Sex', 'male'), ('Pclass', '3'), ('Cabin', '0'), ('Sex', 'female'), ('Pclass', '1')}\n"
     ]
    }
   ],
   "source": [
    "decision_tree = Decision_tree(train_df, 'Survived', 5)\n",
    "gini_dict = decision_tree.all_iteration()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boosting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
